{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui temos o download do FinText baseado no FastText (método Skip-gram). Para uso do FinText baseado no Word2Vec (método Skip-gram), basta que se altere a URL abaixo e as linhas de código que usem o arquivo baixado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KuCd9TxX5lg"
   },
   "outputs": [],
   "source": [
    "!wget https://www.rahimikia.com/FinText/FinText_FastText_Skip-gram.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJn01fNHYeJt"
   },
   "outputs": [],
   "source": [
    "!mkdir FinText_Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VNXEuB6YPd2"
   },
   "outputs": [],
   "source": [
    "!unzip \"/content/FinText_FastText_Skip-gram.zip\" -d \"/content/FinText_FastText/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30zpFuqQvSBc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "rng = np.random.RandomState(42)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(42)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_MdWs_nvTNN"
   },
   "outputs": [],
   "source": [
    "!pip install keras -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GX99FLWhvVwI"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow-addons tensorflow-determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d36jvhW3vXsb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import FastText\n",
    "from tensorflow_addons.optimizers import AdamW\n",
    "from tensorflow.keras.layers import Layer, Embedding, Input, Dropout, Bidirectional, LSTM, GRU, Dense, Conv1D, MaxPooling1D\n",
    "#from tensorflow.compat.v1.keras.layers import CuDNNGRU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, f1_score, precision_score, recall_score, balanced_accuracy_score\n",
    "from imblearn.metrics import specificity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZwsl5tdvkuh"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/content/title-sentiment.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDaOsP9Rwtjy"
   },
   "outputs": [],
   "source": [
    "data.title = data.title.astype(str)\n",
    "data.sentiment = data.sentiment.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1m6yOl1UwwMq"
   },
   "outputs": [],
   "source": [
    "data['title'] = data['title'].str.replace(r'[^\\w\\s]+', '')\n",
    "data['title'] = data['title'].str.replace('\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjX9Jp-1wyjr"
   },
   "outputs": [],
   "source": [
    "X = data['title'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bgf2HZxww1Gx"
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 12697\n",
    "MAX_SEQUENCE_LENGTH = 225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LOQKyzVw3QQ"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS,split=' ')\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a06pzMCPw5kR"
   },
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdKDL0SVw7tv"
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8ZmuBihw-p5"
   },
   "outputs": [],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6recTeZtxBRJ"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print (vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBbvc3qCxDWC"
   },
   "outputs": [],
   "source": [
    "FinText_FastText_skipgram = FastText.load('/content/FinText_FastText/FinText_FastText_Skip-gram/Word_Embedding_2000_2015')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddkE6bVax4bM"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9LNcAYrmx51Q"
   },
   "outputs": [],
   "source": [
    "for word, i in tokenizer.word_index.items():\n",
    "\ttry:\n",
    "\t\tembedding_vector = FinText_FastText_skipgram[word]\n",
    "\t\tif embedding_vector is not None:\n",
    "\t\t\tembedding_matrix[i] = embedding_vector\n",
    "\texcept:\n",
    "\t\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrUY_4AryESv"
   },
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeOL2UaBzS-u"
   },
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEvsbUXJzNZu"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=(MAX_SEQUENCE_LENGTH,), trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KN8RegMFzQGI"
   },
   "outputs": [],
   "source": [
    "custom_adam = AdamW(weight_decay=0.0,learning_rate=1e-5, epsilon=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definições para uso do mecanismo de Atenção nas RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3W7PcZmzY2a"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import regularizers, constraints, initializers\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cc.gatech.edu/~dyang888/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição da RNN (pode ser alterada para definir outras arquiteturas, como LSTMs bidirecionais, GRUs, Convnets, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "novZKLuCza9t"
   },
   "outputs": [],
   "source": [
    "def lstm_model(input_shape):\n",
    "  X_indices = Input(input_shape)\n",
    "  embeddings = embedding_layer(X_indices)\n",
    "  #X = Dropout(0.5)(embeddings)\n",
    "  X = LSTM(50, return_sequences=True)(embeddings)\n",
    "  X = LSTM(50, return_sequences=True)(X)\n",
    "  #X = LSTM(50, return_sequences=False)(X)\n",
    "  X = AttentionWithContext()(X)\n",
    "  X = Dense(3, activation='softmax')(X)\n",
    "  model = Model(inputs=X_indices, outputs=X)\n",
    "  \n",
    "  model.summary()\n",
    "  plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento da RNN com 10-fold cross-validation e obtenção dos valores de benchmarking do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ccU8GW5zloM"
   },
   "outputs": [],
   "source": [
    "bilstm_mcc = []\n",
    "bilstm_f1 = []\n",
    "bilstm_precision = []\n",
    "bilstm_recall = []\n",
    "bilstm_bacc = []\n",
    "bilstm_spec = []\n",
    "\n",
    "fold = 1\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, random_state=rng, shuffle=True)\n",
    "for train_index, test_index in skf.split(X, Y):\n",
    "    model_bilstm = lstm_model((MAX_SEQUENCE_LENGTH,))\n",
    "    model_bilstm.compile(optimizer=custom_adam,loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    class_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(y_train), y = y_train)\n",
    "    weight = {i : class_weights[i] for i in range(3)}\n",
    "    \n",
    "    model_bilstm.fit(X_train,y_train,epochs=10,verbose=1,batch_size=32, class_weight=weight)\n",
    "\n",
    "    y_pred = model_bilstm.predict(X_test, batch_size=32)\n",
    "    preds = np.argmax(y_pred, axis = 1)\n",
    "    \n",
    "    cnf_mtx = confusion_matrix(y_test, preds)\n",
    "    print(\"Fold #%i Confusion Matrix:\" % fold)\n",
    "    print(cnf_mtx)\n",
    "    \n",
    "    bilstm_mcc.append(matthews_corrcoef(y_test, preds))\n",
    "    bilstm_f1.append(f1_score(y_test, preds, average='weighted'))\n",
    "    bilstm_precision.append(precision_score(y_test, preds, average='weighted'))\n",
    "    bilstm_recall.append(recall_score(y_test, preds, average='weighted'))\n",
    "    bilstm_bacc.append(balanced_accuracy_score(y_test, preds))\n",
    "    bilstm_spec.append(specificity_score(y_test, preds, average='weighted'))\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFxPt91e7ItI"
   },
   "outputs": [],
   "source": [
    "print(f\"Mean-MCC: {sum(bilstm_mcc) / len(bilstm_mcc):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-SHj18r7Q1v"
   },
   "outputs": [],
   "source": [
    "print(f\"Mean-F1: {sum(bilstm_f1) / len(bilstm_f1):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6M4RwnXA7SUE"
   },
   "outputs": [],
   "source": [
    "print(f\"Mean-Precision: {sum(bilstm_precision) / len(bilstm_precision):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7dNVyHx7VPc"
   },
   "outputs": [],
   "source": [
    "print(f\"Mean-Recall: {sum(bilstm_recall) / len(bilstm_recall):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAVSiybd7X5B"
   },
   "outputs": [],
   "source": [
    "print(f\"Mean-BACC: {sum(bilstm_bacc) / len(bilstm_bacc):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bzsUROz7ZPm"
   },
   "outputs": [],
   "source": [
    "print(f\"Mean-Specificity: {sum(bilstm_spec) / len(bilstm_spec):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Untitled7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

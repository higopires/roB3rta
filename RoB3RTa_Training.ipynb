{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yf9ZmMFPXOD_"
   },
   "outputs": [],
   "source": [
    "# We won't need TensorFlow here\n",
    "!pip uninstall -y tensorflow\n",
    "# Install `transformers` from master\n",
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip list | grep -E 'transformers|tokenizers'\n",
    "# transformers version at notebook update --- 2.11.0\n",
    "# tokenizers version at notebook update --- 0.8.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UseeMGx1XX1B"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reO4lk6BPq9s"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def remove_empty_lines(filename):\n",
    "    if not os.path.isfile(filename):\n",
    "        print(\"{} does not exist \".format(filename))\n",
    "        return\n",
    "    with open(filename) as filehandle:\n",
    "        lines = filehandle.readlines()\n",
    "\n",
    "    with open(filename, 'w') as filehandle:\n",
    "        lines = filter(lambda x: x.strip(), lines)\n",
    "        filehandle.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus (armazenado no Google Drive, para mais fácil acesso ao arquivo que tem ~ 1GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remoção de linhas em branco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cB4ZNgyDP2AL"
   },
   "outputs": [],
   "source": [
    "remove_empty_lines('/content/drive/MyDrive/Final.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embaralhar linhas (uma vez que as linhas dos tweets foram postas sequencialmente após os textos da Leipzig Collection, essa randomização é importante para que os dados de treinamento contemplem dados das duas fontes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KtE7uwAHErw"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "lines = open('/content/drive/MyDrive/Final.txt').readlines()\n",
    "random.shuffle(lines)\n",
    "open('/content/drive/MyDrive/Final_Shuffled.txt', 'w').writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ElnLczxEOku"
   },
   "outputs": [],
   "source": [
    "!wc -l /content/drive/MyDrive/Final_Shuffled.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "moQqAxe3Ebnf"
   },
   "outputs": [],
   "source": [
    "!shuf -n 5 /content/drive/MyDrive/Final_Shuffled.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAXQwaysNnV-"
   },
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubu47deYNTbV"
   },
   "outputs": [],
   "source": [
    "# Get a subset of first 44362 lines for training\n",
    "#TRAIN_SIZE = 44362 #@param {type:\"integer\"}\n",
    "#!(head -n $TRAIN_SIZE /content/drive/MyDrive/Final_Shuffled.txt) > data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjF80FLROAxF"
   },
   "outputs": [],
   "source": [
    "# Get a subset of next 443 lines for validation\n",
    "#VAL_SIZE = 443 #@param {type:\"integer\"}\n",
    "#!(sed -n {TRAIN_SIZE + 1},{TRAIN_SIZE + VAL_SIZE}p /content/drive/MyDrive/Final_Shuffled.txt) > data/dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsbdoMf3mASs"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "#paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files='/content/drive/MyDrive/Final_Shuffled.txt', vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hl46w21gmV-i"
   },
   "outputs": [],
   "source": [
    "!mkdir roB3rta\n",
    "tokenizer.save_model(\"roB3rta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDHIFGpEmcU0"
   },
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./roB3rta/vocab.json\",\n",
    "    \"./roB3rta/merges.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgBZLQKhmkBe"
   },
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dD8iW5RzmmQK"
   },
   "outputs": [],
   "source": [
    "tokenizer.encode(\"Ações da Bolsa de Valores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcixvbOkmr4a"
   },
   "outputs": [],
   "source": [
    "tokenizer.encode(\"Ações da Bolsa de Valores\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWvo7w8jmxIQ"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=512,\n",
    "    num_attention_heads=8,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNgxkP_km1kt"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./roB3rta\", max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "608w0CIbm6j9"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNYJfyx2m_RS"
   },
   "outputs": [],
   "source": [
    "model.num_parameters()\n",
    "# => 84 million parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJcoprKw0063"
   },
   "outputs": [],
   "source": [
    "!mkdir ./shards\n",
    "!split -a 4 -l 256000 -d /content/drive/MyDrive/Final_Shuffled.txt ./shards/shard_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8JLUm-3WfqQ"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXlir9qyWOFn"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob('shards/*')\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('text', data_files=files, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "McZbtHdvWp3m"
   },
   "outputs": [],
   "source": [
    "def encode(examples):\n",
    "  return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=96)\n",
    "\n",
    "dataset = dataset.map(encode, batched=True)\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7ZZPsQFnGks"
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#from transformers import LineByLineTextDataset\n",
    "#\n",
    "#dataset = LineByLineTextDataset(\n",
    "#    tokenizer=tokenizer,\n",
    "#    file_path=\"/content/drive/MyDrive/Final_Shuffled.txt\",\n",
    "#    block_size=128,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76IhHbcxpaqs"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tENzajnWY9AQ"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./roB3rta\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    fp16=True,\n",
    "    half_precision_backend='amp',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PINs7snxiWcL"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA/apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVNTApqgiqPA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('apex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1zMyzUQmUgu"
   },
   "outputs": [],
   "source": [
    "!pip install AMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tb5MMmAYixWl"
   },
   "outputs": [],
   "source": [
    "!pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQ67oTuXY_sc"
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rx_1feAXZELE"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./roB3rta\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Untitled11.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

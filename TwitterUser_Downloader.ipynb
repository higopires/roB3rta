{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pcZP8xY7TzR"
      },
      "outputs": [],
      "source": [
        "# For sending GET requests from the API\n",
        "import requests\n",
        "# For saving access tokens and for file management when creating and adding to the dataset\n",
        "import os\n",
        "# For dealing with json responses we receive from the API\n",
        "import json\n",
        "# For displaying the data after\n",
        "import pandas as pd\n",
        "# For saving the response data in CSV format\n",
        "import csv\n",
        "# For parsing the dates received from twitter in readable formats\n",
        "import datetime\n",
        "import dateutil.parser\n",
        "import unicodedata\n",
        "#To add wait time between requests\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ticker = '@infomoney'"
      ],
      "metadata": {
        "id": "z6hTK0cwNQ0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TOKEN'] = 'AAAAAAAAAAAAAAAAAAAAAKr2YwEAAAAARcXWQUZErLAaAbZAOaX1zAP8dMg%3DFRcZINkoyzPEUBTFYV7hiBZj4pY86h6PBadQ8xayMast6nsBoK'"
      ],
      "metadata": {
        "id": "QvxRtnSa7Vjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def auth():\n",
        "    return os.getenv('TOKEN')"
      ],
      "metadata": {
        "id": "ybTSIFm07tk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_headers(bearer_token):\n",
        "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
        "    return headers"
      ],
      "metadata": {
        "id": "Snn_UmB07wns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_url(keyword, start_date, end_date, max_results = 10):\n",
        "    \n",
        "    search_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n",
        "\n",
        "    #change params based on the endpoint you are using\n",
        "    query_params = {'query': keyword,\n",
        "                    'start_time': start_date,\n",
        "                    'end_time': end_date,\n",
        "                    'max_results': max_results,\n",
        "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
        "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
        "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
        "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
        "                    'next_token': {}}\n",
        "    return (search_url, query_params)"
      ],
      "metadata": {
        "id": "EDgmHt6a70uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def connect_to_endpoint(url, headers, params, next_token = None):\n",
        "    params['next_token'] = next_token   #params object received from create_url function\n",
        "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
        "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(response.status_code, response.text)\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "pfhaFOPA8XXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Inputs for the request\n",
        "bearer_token = auth()\n",
        "headers = create_headers(bearer_token)\n",
        "keyword = \"{tname} lang:pt\".format(tname = ticker)\n",
        "start_time = \"2006-03-21T00:00:00.000Z\"\n",
        "end_time = \"2022-02-08T00:00:00.000Z\"\n",
        "max_results = 500"
      ],
      "metadata": {
        "id": "rbPaKDzP8YU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword"
      ],
      "metadata": {
        "id": "HducV-0oU9tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = create_url(keyword, start_time,end_time, max_results)\n",
        "\n",
        "json_response = connect_to_endpoint(url[0], headers, url[1])"
      ],
      "metadata": {
        "id": "KcugeRVZ8a_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(json_response, indent=4, sort_keys=True))"
      ],
      "metadata": {
        "id": "YpN-6OJL8mY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_response['data'][0]['created_at']"
      ],
      "metadata": {
        "id": "Pz6umaO68wqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_response['meta']['result_count']"
      ],
      "metadata": {
        "id": "TAIPJ3ew81Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create file\n",
        "csvFile = open(\"{tname}.csv\".format(tname = ticker), \"a\", newline=\"\", encoding='utf-8')\n",
        "csvWriter = csv.writer(csvFile)\n",
        "\n",
        "#Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
        "csvWriter.writerow(['author id', 'created_at', 'geo', 'id','lang', 'like_count', 'quote_count', 'reply_count','retweet_count','source','tweet'])\n",
        "csvFile.close()"
      ],
      "metadata": {
        "id": "SrzTSYYj85L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def append_to_csv(json_response, fileName):\n",
        "\n",
        "    #A counter variable\n",
        "    counter = 0\n",
        "\n",
        "    #Open OR create the target CSV file\n",
        "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
        "    csvWriter = csv.writer(csvFile)\n",
        "\n",
        "    #Loop through each tweet\n",
        "    for tweet in json_response['data']:\n",
        "        \n",
        "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
        "        # So we will account for that\n",
        "\n",
        "        # 1. Author ID\n",
        "        author_id = tweet['author_id']\n",
        "\n",
        "        # 2. Time created\n",
        "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
        "\n",
        "        # 3. Geolocation\n",
        "        if ('geo' in tweet):   \n",
        "            geo = tweet['geo']['place_id']\n",
        "        else:\n",
        "            geo = \" \"\n",
        "\n",
        "        # 4. Tweet ID\n",
        "        tweet_id = tweet['id']\n",
        "\n",
        "        # 5. Language\n",
        "        lang = tweet['lang']\n",
        "\n",
        "        # 6. Tweet metrics\n",
        "        retweet_count = tweet['public_metrics']['retweet_count']\n",
        "        reply_count = tweet['public_metrics']['reply_count']\n",
        "        like_count = tweet['public_metrics']['like_count']\n",
        "        quote_count = tweet['public_metrics']['quote_count']\n",
        "\n",
        "        # 7. source\n",
        "        source = tweet['source']\n",
        "\n",
        "        # 8. Tweet text\n",
        "        text = tweet['text']\n",
        "        \n",
        "        # Assemble all data in a list\n",
        "        res = [author_id, created_at, geo, tweet_id, lang, like_count, quote_count, reply_count, retweet_count, source, text]\n",
        "        \n",
        "        # Append the result to the CSV file\n",
        "        csvWriter.writerow(res)\n",
        "        counter += 1\n",
        "\n",
        "    # When done, close the CSV file\n",
        "    csvFile.close()\n",
        "\n",
        "    # Print the number of tweets for this iteration\n",
        "    print(\"# of Tweets added from this response: \", counter) "
      ],
      "metadata": {
        "id": "oMZCKlOI9Uj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "append_to_csv(json_response, \"{tname}.csv\".format(tname = ticker))"
      ],
      "metadata": {
        "id": "A9_KczU19V42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"{tname}.csv\".format(tname = ticker))"
      ],
      "metadata": {
        "id": "LHDtHKsV9JD3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}